{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Patternic10/CSCI-4170-Project-in-AI-ML/blob/main/Homework5_T3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQXb_DId1KwN",
        "outputId": "a60e3b73-dff8-4c1c-ea89-3ba27a783a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Weights:\n",
            "          0         1         2         3\n",
            "0  0.241597  0.206291  0.168301  0.383810\n",
            "1  0.235459  0.232869  0.231922  0.299750\n",
            "2  0.253643  0.218832  0.178914  0.348611\n",
            "3  0.215477  0.202055  0.211048  0.371421\n",
            "\n",
            "Attention Output:\n",
            "          0         1         2\n",
            "0  0.467834  0.410982  0.558603\n",
            "1  0.506644  0.489666  0.554716\n",
            "2  0.483587  0.433182  0.570991\n",
            "3  0.473400  0.441020  0.529237\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Compute the scaled dot-product attention.\n",
        "\n",
        "    Parameters:\n",
        "    Q (numpy.ndarray): Query matrix of shape (batch_size, seq_len, d_k)\n",
        "    K (numpy.ndarray): Key matrix of shape (batch_size, seq_len, d_k)\n",
        "    V (numpy.ndarray): Value matrix of shape (batch_size, seq_len, d_v)\n",
        "    mask (numpy.ndarray, optional): Masking tensor of shape (batch_size, seq_len, seq_len)\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Output matrix after attention mechanism\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]  # Dimension of key vectors\n",
        "\n",
        "    # Compute attention scores\n",
        "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)  # (batch_size, seq_len, seq_len)\n",
        "\n",
        "    # Apply mask (if any)\n",
        "    if mask is not None:\n",
        "        scores = np.where(mask == 0, -np.inf, scores)\n",
        "\n",
        "    # Softmax to get attention weights\n",
        "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
        "\n",
        "    # Compute output\n",
        "    output = np.matmul(attention_weights, V)  # (batch_size, seq_len, d_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Example usage\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "d_k = 3\n",
        "d_v = 3\n",
        "\n",
        "np.random.seed(42)  # For reproducibility\n",
        "Q = np.random.rand(batch_size, seq_len, d_k)\n",
        "K = np.random.rand(batch_size, seq_len, d_k)\n",
        "V = np.random.rand(batch_size, seq_len, d_v)\n",
        "\n",
        "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Convert results to pandas DataFrame for readability\n",
        "print(\"Attention Weights:\")\n",
        "print(pd.DataFrame(attn_weights[0]))  # Displaying only first batch\n",
        "\n",
        "print(\"\\nAttention Output:\")\n",
        "print(pd.DataFrame(output[0]))  # Displaying only first batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YODfrulm1KwV",
        "outputId": "28af5ab2-2566-4d0a-d710-74eb7d3336ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "[[1.79337538 2.79337538]\n",
            " [2.17124184 3.17124184]\n",
            " [2.         3.        ]]\n",
            "\n",
            "Attention Weights:\n",
            "[[0.47108308 0.26445846 0.26445846]\n",
            " [0.21917211 0.39041395 0.39041395]\n",
            " [0.26445846 0.47108308 0.26445846]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    # Step 1: Compute the dot product of Q and K\n",
        "    dot_product = np.dot(Q, K.T)\n",
        "\n",
        "    # Step 2: Scale the dot product by the square root of the dimensionality of K\n",
        "    d_k = K.shape[-1]\n",
        "    scaled_dot_product = dot_product / np.sqrt(d_k)\n",
        "\n",
        "    # Step 3: Apply the softmax function to obtain the attention weights\n",
        "    attention_weights = np.exp(scaled_dot_product) / np.sum(np.exp(scaled_dot_product), axis=-1, keepdims=True)\n",
        "\n",
        "    # Step 4: Compute the weighted sum of the values using the attention weights\n",
        "    output = np.dot(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Define dummy data\n",
        "Q = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])  # Queries\n",
        "K = np.array([[1, 0, 1], [1, 1, 0], [0, 1, 1]])  # Keys\n",
        "V = np.array([[1, 2], [2, 3], [3, 4]])           # Values\n",
        "\n",
        "# Compute the scaled dot-product attention\n",
        "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"Output:\")\n",
        "print(output)\n",
        "print(\"\\nAttention Weights:\")\n",
        "print(attention_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFtL3_qC1KwW"
      },
      "outputs": [],
      "source": [
        "#Modified Implementation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Initialize weights for the encoder\n",
        "        self.W = np.random.randn(hidden_dim, input_dim) * 0.01  # Input to hidden\n",
        "        self.U = np.random.randn(hidden_dim, hidden_dim) * 0.01  # Hidden to hidden\n",
        "        self.b = np.zeros((hidden_dim, 1))                       # Bias\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "        X: Input sequence of shape (input_dim, seq_len)\n",
        "        Returns: Hidden states of shape (hidden_dim, seq_len)\n",
        "        \"\"\"\n",
        "        #X = X.T\n",
        "        seq_len = X.shape[1]\n",
        "        #print(f\"Shape of X: {X.shape}\")\n",
        "        #print(f\"Seq_len: {seq_len}\")\n",
        "        hidden_states = np.zeros((self.hidden_dim, seq_len))\n",
        "\n",
        "        # Initialize the first hidden state\n",
        "        h_prev = np.zeros((self.hidden_dim, 1))\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = X[:, t].reshape(-1, 1)  # Current input at time step t\n",
        "            # Check shapes\n",
        "            #print(f\"Shape of self.W: {self.W.shape}\")  # Should be (hidden_dim, input_dim)\n",
        "            #print(f\"Shape of x_t: {x_t.shape}\")        # Should be (input_dim, 1)\n",
        "            #print(f\"Shape of self.U: {self.U.shape}\")  # Should be (hidden_dim, hidden_dim)\n",
        "            #print(f\"Shape of h_prev: {h_prev.shape}\")  # Should be (hidden_dim, 1)\n",
        "            h_t = np.tanh(np.dot(self.W, x_t) + np.dot(self.U, h_prev) + self.b)\n",
        "            # Check shape of h_t\n",
        "            #print(f\"Shape of h_t: {h_t.shape}\")  # Should be (hidden_dim, 1)\n",
        "            hidden_states[:, t] = h_t.flatten()\n",
        "            h_prev = h_t\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class DecoderWithAttention:\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        # Initialize weights for the decoder\n",
        "        self.W = np.random.randn(hidden_dim, hidden_dim + hidden_dim) * 0.01  # For combined input\n",
        "        self.U = np.random.randn(hidden_dim, hidden_dim) * 0.01               # Hidden to hidden\n",
        "        self.V = np.random.randn(output_dim, hidden_dim) * 0.01                # Hidden to output\n",
        "        self.b = np.zeros((hidden_dim, 1))                                     # Bias\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        \"\"\"\n",
        "        Scaled dot-product attention.\n",
        "        Q: Query (decoder hidden state) of shape (hidden_dim, 1)\n",
        "        K: Keys (encoder hidden states) of shape (hidden_dim, seq_len)\n",
        "        V: Values (encoder hidden states) of shape (hidden_dim, seq_len)\n",
        "        \"\"\"\n",
        "        d_k = K.shape[0]  # Dimensionality of keys\n",
        "        scores = np.dot(Q.T, K) / np.sqrt(d_k)  # Alignment scores of shape (1, seq_len)\n",
        "        attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)  # Softmax\n",
        "        context = np.dot(attention_weights, V.T)  # Weighted sum of values, shape (1, hidden_dim)\n",
        "        return context.T, attention_weights  # Transpose context to (hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encoder_hidden_states):\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder with attention.\n",
        "        encoder_hidden_states: Hidden states from the encoder (hidden_dim, seq_len)\n",
        "        Returns: Outputs of shape (output_dim, seq_len)\n",
        "        \"\"\"\n",
        "        seq_len = encoder_hidden_states.shape[1]\n",
        "        outputs = np.zeros((self.output_dim, seq_len))\n",
        "        h_prev = np.zeros((self.hidden_dim, 1))  # Initialize decoder hidden state\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            # Compute attention context\n",
        "            context, _ = self.scaled_dot_product_attention(h_prev, encoder_hidden_states, encoder_hidden_states)\n",
        "\n",
        "            # Concatenate context with previous hidden state\n",
        "            combined = np.concatenate((h_prev, context), axis=0)  # Shape (2 * hidden_dim, 1)\n",
        "\n",
        "            # Compute new hidden state\n",
        "            h_t = np.tanh(np.dot(self.W, combined) + np.dot(self.U, h_prev) + self.b)\n",
        "\n",
        "            # Compute output (apply softmax for probabilities)\n",
        "            y_t = np.dot(self.V, h_t)\n",
        "            y_t = np.exp(y_t) / np.sum(np.exp(y_t), axis=0)  # Softmax\n",
        "            outputs[:, t] = y_t.flatten()\n",
        "\n",
        "            # Update previous hidden state\n",
        "            h_prev = h_t\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class Seq2SeqModel:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = DecoderWithAttention(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass for the seq2seq model.\n",
        "        X: Input sequence (input_dim, seq_len)\n",
        "        Returns: Outputs of shape (output_dim, seq_len)\n",
        "        \"\"\"\n",
        "        # Encode the input sequence\n",
        "        encoder_hidden_states = self.encoder.forward(X)\n",
        "\n",
        "        # Decode the target sequence with attention\n",
        "        outputs = self.decoder.forward(encoder_hidden_states)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H46w_ZmJ1KwW"
      },
      "outputs": [],
      "source": [
        "#Modified Implementation Grok\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Initialize weights for the encoder\n",
        "        self.W = np.random.randn(hidden_dim, input_dim) * 0.01  # Input to hidden\n",
        "        self.U = np.random.randn(hidden_dim, hidden_dim) * 0.01  # Hidden to hidden\n",
        "        self.b = np.zeros((hidden_dim, 1))                       # Bias\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "        X: Input sequence of shape (input_dim, seq_len)\n",
        "        Returns: Hidden states of shape (hidden_dim, seq_len)\n",
        "        \"\"\"\n",
        "        #X = X.T\n",
        "        seq_len = X.shape[1]\n",
        "        #print(f\"Shape of X: {X.shape}\")\n",
        "        #print(f\"Seq_len: {seq_len}\")\n",
        "        hidden_states = np.zeros((self.hidden_dim, seq_len))\n",
        "\n",
        "        # Initialize the first hidden state\n",
        "        h_prev = np.zeros((self.hidden_dim, 1))\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = X[:, t].reshape(-1, 1)  # Current input at time step t\n",
        "            # Check shapes\n",
        "            #print(f\"Shape of self.W: {self.W.shape}\")  # Should be (hidden_dim, input_dim)\n",
        "            #print(f\"Shape of x_t: {x_t.shape}\")        # Should be (input_dim, 1)\n",
        "            #print(f\"Shape of self.U: {self.U.shape}\")  # Should be (hidden_dim, hidden_dim)\n",
        "            #print(f\"Shape of h_prev: {h_prev.shape}\")  # Should be (hidden_dim, 1)\n",
        "            h_t = np.tanh(np.dot(self.W, x_t) + np.dot(self.U, h_prev) + self.b)\n",
        "            # Check shape of h_t\n",
        "            #print(f\"Shape of h_t: {h_t.shape}\")  # Should be (hidden_dim, 1)\n",
        "            hidden_states[:, t] = h_t.flatten()\n",
        "            h_prev = h_t\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class DecoderWithAttention:\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        # Initialize weights for the decoder\n",
        "        self.W = np.random.randn(hidden_dim, hidden_dim + hidden_dim) * 0.01  # For combined input\n",
        "        self.U = np.random.randn(hidden_dim, hidden_dim) * 0.01               # Hidden to hidden\n",
        "        self.V = np.random.randn(output_dim, hidden_dim) * 0.01                # Hidden to output\n",
        "        self.b = np.zeros((hidden_dim, 1))                                     # Bias\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        \"\"\"\n",
        "        Scaled dot-product attention.\n",
        "        Q: Query (decoder hidden state) of shape (hidden_dim, 1)\n",
        "        K: Keys (encoder hidden states) of shape (hidden_dim, seq_len)\n",
        "        V: Values (encoder hidden states) of shape (hidden_dim, seq_len)\n",
        "        \"\"\"\n",
        "        d_k = K.shape[0]  # Dimensionality of keys\n",
        "        scores = np.dot(Q.T, K) / np.sqrt(d_k)  # Alignment scores of shape (1, seq_len)\n",
        "        attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)  # Softmax\n",
        "        context = np.dot(attention_weights, V.T)  # Weighted sum of values, shape (1, hidden_dim)\n",
        "        return context.T, attention_weights  # Transpose context to (hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encoder_hidden_states):\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder with attention.\n",
        "        encoder_hidden_states: Hidden states from the encoder (hidden_dim, seq_len)\n",
        "        Returns: Outputs of shape (output_dim, seq_len)\n",
        "        \"\"\"\n",
        "        seq_len = encoder_hidden_states.shape[1]\n",
        "        outputs = np.zeros((self.output_dim, seq_len))\n",
        "        h_prev = np.zeros((self.hidden_dim, 1))  # Initialize decoder hidden state\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            # Compute attention context\n",
        "            context, _ = self.scaled_dot_product_attention(h_prev, encoder_hidden_states, encoder_hidden_states)\n",
        "\n",
        "            # Concatenate context with previous hidden state\n",
        "            combined = np.concatenate((h_prev, context), axis=0)  # Shape (2 * hidden_dim, 1)\n",
        "\n",
        "            # Compute new hidden state\n",
        "            h_t = np.tanh(np.dot(self.W, combined) + np.dot(self.U, h_prev) + self.b)\n",
        "\n",
        "            # Compute output (apply softmax for probabilities)\n",
        "            y_t = np.dot(self.V, h_t)\n",
        "            y_t = np.exp(y_t) / np.sum(np.exp(y_t), axis=0)  # Softmax\n",
        "            outputs[:, t] = y_t.flatten()\n",
        "\n",
        "            # Update previous hidden state\n",
        "            h_prev = h_t\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class Seq2SeqModel:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = DecoderWithAttention(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X, Y=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the seq2seq model.\n",
        "        X: Input sequence (input_dim, seq_len)\n",
        "        Returns: Outputs of shape (output_dim, seq_len)\n",
        "        \"\"\"\n",
        "        # Encode the input sequence\n",
        "        encoder_hidden_states = self.encoder.forward(X)\n",
        "\n",
        "        # Decode the target sequence with attention\n",
        "        outputs = self.decoder.forward(encoder_hidden_states,Y)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN1f2T231KwX"
      },
      "outputs": [],
      "source": [
        "#New implementation\n",
        "import numpy as np\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.W = np.random.randn(hidden_dim, input_dim) * 0.01\n",
        "        self.U = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
        "        self.b = np.zeros((hidden_dim, 1))\n",
        "\n",
        "    def forward(self, X):\n",
        "        seq_len = X.shape[1]\n",
        "        hidden_states = np.zeros((self.hidden_dim, seq_len))\n",
        "        h_prev = np.zeros((self.hidden_dim, 1))\n",
        "        for t in range(seq_len):\n",
        "            x_t = X[:, t].reshape(-1, 1)\n",
        "            h_t = np.tanh(np.dot(self.W, x_t) + np.dot(self.U, h_prev) + self.b)\n",
        "            hidden_states[:, t] = h_t.flatten()\n",
        "            h_prev = h_t\n",
        "        return hidden_states\n",
        "\n",
        "class DecoderWithAttention:\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = np.random.randn(hidden_dim, output_dim) * 0.01\n",
        "        self.W = np.random.randn(hidden_dim, hidden_dim + hidden_dim) * 0.01\n",
        "        self.U = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
        "        self.V = np.random.randn(output_dim, hidden_dim) * 0.01\n",
        "        self.b = np.zeros((hidden_dim, 1))\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        d_k = K.shape[0]\n",
        "        scores = np.dot(Q.T, K) / np.sqrt(d_k)\n",
        "        attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
        "        context = np.dot(attention_weights, V.T).T\n",
        "        return context, attention_weights\n",
        "\n",
        "    def forward(self, encoder_hidden_states, Y=None):\n",
        "        if Y is not None:  # Training with teacher forcing\n",
        "            seq_len = len(Y)\n",
        "            outputs = np.zeros((self.output_dim, seq_len))\n",
        "            h_prev = np.zeros((self.hidden_dim, 1))\n",
        "            for t in range(seq_len):\n",
        "                embedded_y = np.zeros((self.hidden_dim, 1)) if t == 0 else self.embedding[:, Y[t-1]].reshape(-1, 1)\n",
        "                context, _ = self.scaled_dot_product_attention(h_prev, encoder_hidden_states, encoder_hidden_states)\n",
        "                combined = np.concatenate((embedded_y, context), axis=0)\n",
        "                h_t = np.tanh(np.dot(self.W, combined) + np.dot(self.U, h_prev) + self.b)\n",
        "                y_t = np.dot(self.V, h_t)\n",
        "                y_t = np.exp(y_t) / np.sum(np.exp(y_t), axis=0)\n",
        "                outputs[:, t] = y_t.flatten()\n",
        "                h_prev = h_t\n",
        "            return outputs\n",
        "        else:  # Inference\n",
        "            seq_len = encoder_hidden_states.shape[1]\n",
        "            outputs = np.zeros((self.output_dim, seq_len))\n",
        "            h_prev = np.zeros((self.hidden_dim, 1))\n",
        "            embedded_y = np.zeros((self.hidden_dim, 1))\n",
        "            for t in range(seq_len):\n",
        "                context, _ = self.scaled_dot_product_attention(h_prev, encoder_hidden_states, encoder_hidden_states)\n",
        "                combined = np.concatenate((embedded_y, context), axis=0)\n",
        "                h_t = np.tanh(np.dot(self.W, combined) + np.dot(self.U, h_prev) + self.b)\n",
        "                y_t = np.dot(self.V, h_t)\n",
        "                y_t = np.exp(y_t) / np.sum(np.exp(y_t), axis=0)\n",
        "                outputs[:, t] = y_t.flatten()\n",
        "                y_pred = np.argmax(y_t)\n",
        "                embedded_y = self.embedding[:, y_pred].reshape(-1, 1)\n",
        "                h_prev = h_t\n",
        "            return outputs\n",
        "\n",
        "class Seq2SeqModel:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = DecoderWithAttention(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X, Y=None):\n",
        "        encoder_hidden_states = self.encoder.forward(X)\n",
        "        outputs = self.decoder.forward(encoder_hidden_states, Y)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sVWkVgF1KwX",
        "outputId": "a41c4f10-9f19-4564-a366-703a962dcdf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Weights:\n",
            "          0         1         2         3\n",
            "0  0.287783  0.209760  0.257723  0.244734\n",
            "1  0.277640  0.220422  0.255693  0.246245\n",
            "2  0.264978  0.233741  0.253151  0.248131\n",
            "3  0.317726  0.180831  0.262175  0.239268\n",
            "\n",
            "Context Vector:\n",
            "          0         1         2\n",
            "0  0.744479  0.764446  0.721462\n",
            "1  0.740357  0.758494  0.719004\n",
            "2  0.735207  0.751058  0.715934\n",
            "3  0.755938  0.780848  0.728558\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Compute the scaled dot-product attention.\n",
        "\n",
        "    Parameters:\n",
        "    Q (numpy.ndarray): Query matrix of shape (batch_size, seq_len, d_k)\n",
        "    K (numpy.ndarray): Key matrix of shape (batch_size, seq_len, d_k)\n",
        "    V (numpy.ndarray): Value matrix of shape (batch_size, seq_len, d_v)\n",
        "    mask (numpy.ndarray, optional): Masking tensor of shape (batch_size, seq_len, seq_len)\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Output matrix after attention mechanism\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]  # Dimension of key vectors\n",
        "\n",
        "    # Compute attention scores\n",
        "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)  # (batch_size, seq_len, seq_len)\n",
        "\n",
        "    # Apply mask (if any)\n",
        "    if mask is not None:\n",
        "        scores = np.where(mask == 0, -np.inf, scores)\n",
        "\n",
        "    # Softmax to get attention weights\n",
        "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
        "\n",
        "    # Compute output\n",
        "    output = np.matmul(attention_weights, V)  # (batch_size, seq_len, d_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "def encoder(X, W):\n",
        "    \"\"\" Simple encoder that transforms input X using learned weights W \"\"\"\n",
        "    return np.tanh(np.dot(X, W))\n",
        "\n",
        "def decoder(Y, W, encoder_outputs):\n",
        "    \"\"\" Simple decoder with attention \"\"\"\n",
        "    Q = np.dot(Y, W)  # Convert decoder input into query\n",
        "    context_vector, attention_weights = scaled_dot_product_attention(Q, encoder_outputs, encoder_outputs)\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "# Example usage\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "d_k = 3\n",
        "d_v = 3\n",
        "hidden_dim = 3\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(batch_size, seq_len, d_k)  # Encoder input\n",
        "Y = np.random.rand(batch_size, seq_len, d_k)  # Decoder input\n",
        "W_enc = np.random.rand(d_k, hidden_dim)  # Encoder weights\n",
        "W_dec = np.random.rand(d_k, hidden_dim)  # Decoder weights\n",
        "\n",
        "encoder_outputs = encoder(X, W_enc)\n",
        "context_vector, attn_weights = decoder(Y, W_dec, encoder_outputs)\n",
        "\n",
        "print(\"Attention Weights:\")\n",
        "print(pd.DataFrame(attn_weights[0]))  # Displaying only first batch\n",
        "\n",
        "print(\"\\nContext Vector:\")\n",
        "print(pd.DataFrame(context_vector[0]))  # Displaying only first batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xstBZdqg1KwY",
        "outputId": "048d667e-24c9-427b-d97e-72b67b771209"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kazeej/anaconda3/envs/RNN/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[278, 10, 4, 2961, 5]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "import spacy\n",
        "import jinja2\n",
        "# Define tokenizers for English and German\n",
        "SRC_LANGUAGE = 'en'  # Source language (English)\n",
        "TGT_LANGUAGE = 'de'  # Target language (German)\n",
        "\n",
        "# Load tokenizers\n",
        "tokenizer_src = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "tokenizer_tgt = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "\n",
        "# Load the dataset\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# Build vocabulary\n",
        "def yield_tokens(data_iter, tokenizer, index):\n",
        "    for src_tgt_pair in data_iter:\n",
        "        yield tokenizer(src_tgt_pair[index])\n",
        "\n",
        "# Build source and target vocabularies\n",
        "vocab_src = build_vocab_from_iterator(yield_tokens(train_iter, tokenizer_src, 0), specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "vocab_tgt = build_vocab_from_iterator(yield_tokens(train_iter, tokenizer_tgt, 1), specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "# Set default index for unknown tokens\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])\n",
        "# Function to tokenize and numericalize a sentence\n",
        "def preprocess_sentence(sentence, tokenizer, vocab):\n",
        "    tokens = tokenizer(sentence)  # Tokenize the sentence\n",
        "    numericalized_tokens = vocab(tokens)  # Convert tokens to numerical indices\n",
        "    return numericalized_tokens\n",
        "# Function to add special tokens\n",
        "def add_special_tokens(tokens, vocab):\n",
        "    sos_token = vocab['<sos>']\n",
        "    eos_token = vocab['<eos>']\n",
        "    return [sos_token] + tokens + [eos_token]\n",
        "\n",
        "# Prepare the dataset\n",
        "def prepare_dataset(data_iter, tokenizer_src, tokenizer_tgt, vocab_src, vocab_tgt):\n",
        "    src_data = []\n",
        "    tgt_data = []\n",
        "    for src, tgt in data_iter:\n",
        "        src_tokens = preprocess_sentence(src, tokenizer_src, vocab_src)\n",
        "        tgt_tokens = preprocess_sentence(tgt, tokenizer_tgt, vocab_tgt)\n",
        "        src_tokens = add_special_tokens(src_tokens, vocab_src)\n",
        "        tgt_tokens = add_special_tokens(tgt_tokens, vocab_tgt)\n",
        "        src_data.append(src_tokens)\n",
        "        tgt_data.append(tgt_tokens)\n",
        "    return src_data, tgt_data\n",
        "\n",
        "# Example usage\n",
        "example_sentence = \"This is a test.\"\n",
        "preprocessed_sentence = preprocess_sentence(example_sentence, tokenizer_src, vocab_src)\n",
        "print(preprocessed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2khJQ4ch1KwY",
        "outputId": "04a3c88c-f50d-4190-bae1-8655c300ce0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 278, 10, 4, 2961, 5, 3]\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "preprocessed_sentence_with_tokens = add_special_tokens(preprocessed_sentence, vocab_src)\n",
        "print(preprocessed_sentence_with_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFJxnJc31KwZ"
      },
      "outputs": [],
      "source": [
        "src_data, tgt_data = prepare_dataset(train_iter, tokenizer_src, tokenizer_tgt, vocab_src, vocab_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RvVAI6z1Kwa",
        "outputId": "8ed7859a-3b1d-44ce-d07b-eeb9879a3b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample preprocessed source sentence: (43,)\n",
            "Sample preprocessed target sentence: (46,)\n"
          ]
        }
      ],
      "source": [
        "# Function to pad sequences\n",
        "def pad_sequence(sequence, max_len, pad_token):\n",
        "    return sequence + [pad_token] * (max_len - len(sequence))\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_src_len = max(len(seq) for seq in src_data)\n",
        "max_tgt_len = max(len(seq) for seq in tgt_data)\n",
        "\n",
        "# Pad all sequences\n",
        "src_data_padded = [pad_sequence(seq, max_src_len, vocab_src['<pad>']) for seq in src_data]\n",
        "tgt_data_padded = [pad_sequence(seq, max_tgt_len, vocab_tgt['<pad>']) for seq in tgt_data]\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "src_data_padded = np.array(src_data_padded)\n",
        "tgt_data_padded = np.array(tgt_data_padded)\n",
        "\n",
        "# Print sample output\n",
        "print(\"Sample preprocessed source sentence:\", src_data_padded[0].shape)\n",
        "print(\"Sample preprocessed target sentence:\", tgt_data_padded[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvZRlq0c1Kwa",
        "outputId": "b10ecf94-5abf-4e3a-fe9c-a50a0f970939"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((29001, 43), (29001, 46))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_data_padded.shape, tgt_data_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbkWHLJ81Kwb",
        "outputId": "cd8a9e2d-2d22-4c46-853b-4b5c004555c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10837, 19214)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab_src), len(vocab_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqR2vz5r1Kwb",
        "outputId": "c89180ca-eac4-4382-a2c6-cee425c0b04e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample one-hot encoded source sentence shape: (10837, 43)\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Function to one-hot encode a sequence of word indices using sparse matrices\n",
        "def one_hot_encode_sparse(sequence, vocab_size):\n",
        "    \"\"\"\n",
        "    Converts a sequence of word indices to a sparse one-hot encoded matrix.\n",
        "    sequence: List of word indices (shape: (seq_len,))\n",
        "    vocab_size: Size of the vocabulary (input_dim)\n",
        "    Returns: Sparse one-hot encoded matrix of shape (vocab_size, seq_len)\n",
        "    \"\"\"\n",
        "    seq_len = len(sequence)\n",
        "    rows = sequence  # Row indices (word indices)\n",
        "    cols = np.arange(seq_len)  # Column indices (time steps)\n",
        "    data = np.ones(seq_len)  # Values (all 1s)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(vocab_size, seq_len))\n",
        "\n",
        "# One-hot encode the padded source sequences using sparse matrices\n",
        "input_dim = len(vocab_src)  # Size of source vocabulary\n",
        "src_data_one_hot = [one_hot_encode_sparse(seq, input_dim) for seq in src_data_padded]\n",
        "\n",
        "# Print sample output\n",
        "print(\"Sample one-hot encoded source sentence shape:\", src_data_one_hot[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GhnXO4Q1Kwc",
        "outputId": "abab15a9-2da8-4a55-8e7e-727afd161e1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((10837, 43), (46,))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_data_one_hot[0].shape, tgt_data_padded[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9p0HfqT1Kwc"
      },
      "outputs": [],
      "source": [
        "def batch_generator(data, batch_size):\n",
        "    \"\"\"\n",
        "    Yields batches of data one at a time.\n",
        "    data: List or array of sequences\n",
        "    batch_size: Number of samples per batch\n",
        "    \"\"\"\n",
        "    num_samples = len(data)\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        yield data[i:i + batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmMFAU9u1Kwc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10  # You can increase this later\n",
        "\n",
        "# Extract model dimensions\n",
        "input_dim = len(vocab_src)   # Size of input vocabulary\n",
        "output_dim = len(vocab_tgt)  # Size of output vocabulary\n",
        "hidden_dim = 128             # You can adjust based on performance\n",
        "\n",
        "# Initialize Seq2Seq model\n",
        "model = Seq2SeqModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define loss function (categorical cross-entropy)\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Computes categorical cross-entropy loss.\n",
        "    y_pred: (output_dim, seq_len) - Softmax probabilities\n",
        "    y_true: (seq_len,) - True target indices\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "    seq_len = len(y_true)\n",
        "    for t in range(seq_len):\n",
        "        loss -= np.log(y_pred[y_true[t], t] + 1e-9)  # Avoid log(0) issues\n",
        "    return loss / seq_len  # Average over sequence length\n",
        "\n",
        "# Compute gradients using finite difference method\n",
        "def compute_gradients(model, X, Y):\n",
        "    \"\"\"\n",
        "    Compute gradients for the model parameters using simple finite difference method.\n",
        "    X: Input sequence (padded source sentence, shape: input_dim, seq_len_src)\n",
        "    Y: Target sequence (padded target sentence, shape: seq_len_tgt)\n",
        "    \"\"\"\n",
        "    epsilon = 1e-5  # Small perturbation\n",
        "    gradients = {}  # Store gradients\n",
        "\n",
        "    # Compute initial loss with teacher forcing\n",
        "    outputs = model.forward(X, Y)\n",
        "    initial_loss = cross_entropy_loss(outputs, Y)\n",
        "\n",
        "    # Encoder parameters to update\n",
        "    encoder_params = ['W', 'U', 'b']\n",
        "    # Decoder parameters to update, including the new embedding matrix\n",
        "    decoder_params = ['embedding', 'W', 'U', 'V', 'b']\n",
        "\n",
        "    # Compute gradients for encoder parameters\n",
        "    for param_name in encoder_params:\n",
        "        param = getattr(model.encoder, param_name)\n",
        "        gradient_matrix = np.zeros_like(param)\n",
        "        for i in range(param.shape[0]):\n",
        "            for j in range(param.shape[1]):\n",
        "                param[i, j] += epsilon\n",
        "                perturbed_outputs = model.forward(X, Y)\n",
        "                perturbed_loss = cross_entropy_loss(perturbed_outputs, Y)\n",
        "                gradient_matrix[i, j] = (perturbed_loss - initial_loss) / epsilon\n",
        "                param[i, j] -= epsilon\n",
        "        gradients[f'encoder_{param_name}'] = gradient_matrix\n",
        "\n",
        "    # Compute gradients for decoder parameters\n",
        "    for param_name in decoder_params:\n",
        "        param = getattr(model.decoder, param_name)\n",
        "        gradient_matrix = np.zeros_like(param)\n",
        "        for i in range(param.shape[0]):\n",
        "            for j in range(param.shape[1]):\n",
        "                param[i, j] += epsilon\n",
        "                perturbed_outputs = model.forward(X, Y)\n",
        "                perturbed_loss = cross_entropy_loss(perturbed_outputs, Y)\n",
        "                gradient_matrix[i, j] = (perturbed_loss - initial_loss) / epsilon\n",
        "                param[i, j] -= epsilon\n",
        "        gradients[f'decoder_{param_name}'] = gradient_matrix\n",
        "\n",
        "    return gradients\n",
        "\n",
        "# Training loop with sparse matrices\n",
        "batch_size = 16  # Number of samples per batch\n",
        "num_epochs = 10  # Number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # Create batch generators for source and target data\n",
        "    src_batches = batch_generator(src_data_one_hot, batch_size)\n",
        "    tgt_batches = batch_generator(tgt_data_padded, batch_size)\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch_idx, (src_batch, tgt_batch) in enumerate(zip(src_batches, tgt_batches)):\n",
        "        batch_loss = 0\n",
        "\n",
        "        # Process each sample in the batch\n",
        "        for i in range(len(src_batch)):\n",
        "            # Get sparse one-hot encoded input and target sequence\n",
        "            X_sparse = src_batch[i]  # Sparse matrix of shape (input_dim, seq_len)\n",
        "            Y = tgt_batch[i]         # Shape: (seq_len,)\n",
        "\n",
        "            # Convert sparse matrix to dense array for model.forward\n",
        "            X = X_sparse.toarray()  # Shape: (input_dim, seq_len)\n",
        "\n",
        "            # Forward pass with teacher forcing\n",
        "            outputs = model.forward(X, Y)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = cross_entropy_loss(outputs, Y)\n",
        "            batch_loss += loss\n",
        "\n",
        "            # Compute gradients\n",
        "            gradients = compute_gradients(model, X, Y)\n",
        "\n",
        "            # Update model parameters using simple gradient descent\n",
        "            # Update encoder parameters\n",
        "            for param_name in ['W', 'U', 'b']:\n",
        "                param = getattr(model.encoder, param_name)\n",
        "                param -= learning_rate * gradients[f'encoder_{param_name}']\n",
        "            # Update decoder parameters\n",
        "            for param_name in ['embedding', 'W', 'U', 'V', 'b']:\n",
        "                param = getattr(model.decoder, param_name)\n",
        "                param -= learning_rate * gradients[f'decoder_{param_name}']\n",
        "\n",
        "        # Accumulate total loss (average loss per sample in the batch)\n",
        "        total_loss += batch_loss / len(src_batch)\n",
        "\n",
        "        # Print batch loss\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}, Loss: {batch_loss / len(src_batch):.4f}\")\n",
        "\n",
        "    # Print average loss per epoch\n",
        "    avg_loss = total_loss / (len(src_data_one_hot) // batch_size)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete! ðŸš€\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmP0ir6D1Kwd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZRhuPvS1Kwe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCbfP1_21Kwe"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpDM5XPJ1Kwe"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attention = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attention, v)\n",
        "    return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAp117Kv1Kwe"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        seq_len = q.size(1)\n",
        "        #print(f\"Input shapes - q: {q.shape}, k: {k.shape}, v: {v.shape}\")  # Debug statement\n",
        "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        scores, attention = scaled_dot_product_attention(q, k, v, mask)\n",
        "        concat = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.wo(concat)\n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQBpXhvX1Kwf"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + ff_output)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + attn_output)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + ff_output)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OpSEbHX1Kwf"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        src = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src, src_mask)\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt = layer(tgt, src, src_mask, tgt_mask)\n",
        "        output = self.fc_out(tgt)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXgJkH3K1Kwf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "from os.path import exists\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "#from torchtext.datasets import Multi30k\n",
        "import spacy\n",
        "from itertools import chain\n",
        "\n",
        "#Load data\n",
        "all_de_train = iter(io.open('Multi30k/train.de', encoding=\"utf8\"))\n",
        "all_en_train = iter(io.open('Multi30k/train.en', encoding=\"utf8\"))\n",
        "all_de_test = iter(io.open('Multi30k/test.de', encoding=\"utf8\"))\n",
        "all_en_test = iter(io.open('Multi30k/test.en', encoding=\"utf8\"))\n",
        "all_de_val = iter(io.open('Multi30k/val.de', encoding=\"utf8\"))\n",
        "all_en_val = iter(io.open('Multi30k/val.en', encoding=\"utf8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfbqtwYg1Kwf"
      },
      "outputs": [],
      "source": [
        "all_de_train = list(all_de_train)\n",
        "all_en_train = list(all_en_train)\n",
        "all_de_test = list(all_de_test)\n",
        "all_en_test = list(all_en_test)\n",
        "all_de_val = list(all_de_val)\n",
        "all_en_val = list(all_en_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "092EbzPF1Kwg"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Define tokenizers for English and German\n",
        "tokenizer_src = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "tokenizer_tgt = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "\n",
        "# Combine all data splits for vocabulary building\n",
        "all_en_data = chain(all_en_train, all_en_val, all_en_test)\n",
        "all_de_data = chain(all_de_train, all_de_val, all_de_test)\n",
        "\n",
        "# Build vocabulary\n",
        "def yield_tokens(data_iter, tokenizer):\n",
        "    for sentence in data_iter:\n",
        "        yield tokenizer(sentence)\n",
        "\n",
        "# Build source and target vocabularies using all data\n",
        "vocab_src = build_vocab_from_iterator(yield_tokens(all_en_data, tokenizer_src), specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "vocab_tgt = build_vocab_from_iterator(yield_tokens(all_de_data, tokenizer_tgt), specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
        "\n",
        "# Set default index for unknown tokens\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omq4tgQQ1Kwg"
      },
      "outputs": [],
      "source": [
        "# Function to tokenize and numericalize a sentence\n",
        "def preprocess_sentence(sentence, tokenizer, vocab):\n",
        "    tokens = tokenizer(sentence)  # Tokenize the sentence\n",
        "    numericalized_tokens = vocab(tokens)  # Convert tokens to numerical indices\n",
        "    return numericalized_tokens\n",
        "\n",
        "# Function to add special tokens\n",
        "def add_special_tokens(tokens, vocab):\n",
        "    sos_token = vocab['<sos>']\n",
        "    eos_token = vocab['<eos>']\n",
        "    return [sos_token] + tokens + [eos_token]\n",
        "\n",
        "# Prepare the dataset\n",
        "def prepare_dataset(src_iter, tgt_iter, tokenizer_src, tokenizer_tgt, vocab_src, vocab_tgt):\n",
        "    src_data = []\n",
        "    tgt_data = []\n",
        "    for src, tgt in zip(src_iter, tgt_iter):\n",
        "        src_tokens = preprocess_sentence(src, tokenizer_src, vocab_src)\n",
        "        tgt_tokens = preprocess_sentence(tgt, tokenizer_tgt, vocab_tgt)\n",
        "        src_tokens = add_special_tokens(src_tokens, vocab_src)\n",
        "        tgt_tokens = add_special_tokens(tgt_tokens, vocab_tgt)\n",
        "        src_data.append(src_tokens)\n",
        "        tgt_data.append(tgt_tokens)\n",
        "    return src_data, tgt_data\n",
        "\n",
        "# Prepare training, validation, and test datasets\n",
        "train_src, train_tgt = prepare_dataset(all_en_train, all_de_train, tokenizer_src, tokenizer_tgt, vocab_src, vocab_tgt)\n",
        "val_src, val_tgt = prepare_dataset(all_en_val, all_de_val, tokenizer_src, tokenizer_tgt, vocab_src, vocab_tgt)\n",
        "test_src, test_tgt = prepare_dataset(all_en_test, all_de_test, tokenizer_src, tokenizer_tgt, vocab_src, vocab_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsoC_d7L1Kwg"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define collate function\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src, tgt in batch:\n",
        "        src_batch.append(torch.tensor(src, dtype=torch.long))\n",
        "        tgt_batch.append(torch.tensor(tgt, dtype=torch.long))\n",
        "    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(list(zip(train_src, train_tgt)), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(list(zip(val_src, val_tgt)), batch_size=batch_size, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(list(zip(test_src, test_tgt)), batch_size=batch_size, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b9uTtXF1Kwg"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "src_vocab_size = len(vocab_src)\n",
        "tgt_vocab_size = len(vocab_tgt)\n",
        "d_model = 64\n",
        "num_heads = 2\n",
        "num_layers = 2\n",
        "d_ff = 128\n",
        "max_len = 100\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoTYrN9g1Kwh"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab_tgt['<pad>'])\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Training function\n",
        "def train(model, train_loader, optimizer, criterion, pad_idx):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        #print(f\"src shape: {src.shape}, tgt shape: {tgt.shape}\")\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        # Prepare tgt_input and tgt_out\n",
        "        tgt_input = tgt[:, :-1]  # Exclude last token\n",
        "        tgt_out = tgt[:, 1:]     # Exclude first token\n",
        "        pad_idx = vocab_tgt['<pad>']\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(src, tgt_input, src_mask=None, tgt_mask=None)  # Pass padding index\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        tgt_out = tgt_out.contiguous().view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, tgt_out)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN852o231Kwh",
        "outputId": "db0a357d-3075-49c0-b68b-65c3af08d814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30: Train Loss = 2.088, Validation Loss = 2.157\n",
            "Epoch 2/30: Train Loss = 2.027, Validation Loss = 2.098\n",
            "Epoch 3/30: Train Loss = 1.968, Validation Loss = 2.044\n",
            "Epoch 4/30: Train Loss = 1.908, Validation Loss = 1.994\n",
            "Epoch 5/30: Train Loss = 1.853, Validation Loss = 1.940\n",
            "Epoch 6/30: Train Loss = 1.796, Validation Loss = 1.892\n",
            "Epoch 7/30: Train Loss = 1.748, Validation Loss = 1.845\n",
            "Epoch 8/30: Train Loss = 1.694, Validation Loss = 1.798\n",
            "Epoch 9/30: Train Loss = 1.646, Validation Loss = 1.762\n",
            "Epoch 10/30: Train Loss = 1.599, Validation Loss = 1.719\n",
            "Epoch 11/30: Train Loss = 1.555, Validation Loss = 1.683\n",
            "Epoch 12/30: Train Loss = 1.510, Validation Loss = 1.640\n",
            "Epoch 13/30: Train Loss = 1.469, Validation Loss = 1.607\n",
            "Epoch 14/30: Train Loss = 1.425, Validation Loss = 1.572\n",
            "Epoch 15/30: Train Loss = 1.386, Validation Loss = 1.542\n",
            "Epoch 16/30: Train Loss = 1.351, Validation Loss = 1.512\n",
            "Epoch 17/30: Train Loss = 1.315, Validation Loss = 1.480\n",
            "Epoch 18/30: Train Loss = 1.278, Validation Loss = 1.463\n",
            "Epoch 19/30: Train Loss = 1.242, Validation Loss = 1.423\n",
            "Epoch 20/30: Train Loss = 1.208, Validation Loss = 1.400\n",
            "Epoch 21/30: Train Loss = 1.176, Validation Loss = 1.375\n",
            "Epoch 22/30: Train Loss = 1.147, Validation Loss = 1.353\n",
            "Epoch 23/30: Train Loss = 1.115, Validation Loss = 1.322\n",
            "Epoch 24/30: Train Loss = 1.085, Validation Loss = 1.308\n",
            "Epoch 25/30: Train Loss = 1.060, Validation Loss = 1.287\n",
            "Epoch 26/30: Train Loss = 1.030, Validation Loss = 1.268\n",
            "Epoch 27/30: Train Loss = 1.003, Validation Loss = 1.252\n",
            "Epoch 28/30: Train Loss = 0.976, Validation Loss = 1.227\n",
            "Epoch 29/30: Train Loss = 0.955, Validation Loss = 1.214\n",
            "Epoch 30/30: Train Loss = 0.926, Validation Loss = 1.196\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "NUM_EPOCHS = 30\n",
        "train_losses = []  # Store training loss values for each epoch\n",
        "val_losses = []    # Store validation loss values for each epoch\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    # Training Loop\n",
        "    model.train()\n",
        "    for src, tgt in train_loader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(src, tgt[:, :-1], src_mask=None, tgt_mask=None)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_loader:\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, tgt[:, :-1], src_mask=None, tgt_mask=None)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, tgt)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Average losses\n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Append the loss values to the lists\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Print the loss at the end of each epoch\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}: Train Loss = {train_loss:.3f}, Validation Loss = {val_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhqnzwxl1Kwh",
        "outputId": "2289a017-7644-4a1e-90d7-6c6632227cce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss', marker='o')\n",
        "plt.plot(val_losses, label='Validation Loss', marker='o')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osJdX-9r1Kwh",
        "outputId": "e8839150-6fce-4b89-a6ee-e9b5229aca39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.3825\n"
          ]
        }
      ],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def evaluate_bleu(model, val_loader, vocab_tgt, device):\n",
        "    model.eval()\n",
        "    translations = []\n",
        "    references = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_loader:\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            # Generate predictions (greedy decoding)\n",
        "            output = model(src, tgt[:, :-1], src_mask=None, tgt_mask=None)\n",
        "            output = output.argmax(dim=-1)  # Greedy decoding\n",
        "\n",
        "            # Convert predictions and references to text\n",
        "            for i in range(output.size(0)):\n",
        "                pred = output[i].tolist()\n",
        "                ref = tgt[i, 1:].tolist()\n",
        "\n",
        "                # Remove padding tokens\n",
        "                pred = [token for token in pred if token != vocab_tgt['<pad>']]\n",
        "                ref = [token for token in ref if token != vocab_tgt['<pad>']]\n",
        "\n",
        "                # Convert tokens to strings\n",
        "                pred_str = ' '.join([vocab_tgt.get_itos()[token] for token in pred])\n",
        "                ref_str = ' '.join([vocab_tgt.get_itos()[token] for token in ref])\n",
        "\n",
        "                translations.append(pred_str.split())\n",
        "                references.append([ref_str.split()])  # Note: BLEU expects a list of references\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu = bleu_score(translations, references)\n",
        "    return bleu\n",
        "\n",
        "# Compute BLEU score on the validation set\n",
        "bleu_score = evaluate_bleu(model, val_loader, vocab_tgt, device)\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2g9vIQC1Kwi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "RNN",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}