{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Patternic10/CSCI-4170-Project-in-AI-ML/blob/main/Homework5_T3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWkKS-HOPkkH",
        "outputId": "10ef971a-103a-4d07-b2b3-58d456ac6a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images shape: torch.Size([8, 3, 256, 256])\n",
            "Masks shape: torch.Size([8, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import tifffile  # For loading .tif files\n",
        "import cv2\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "class PolypDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Path to the directory with input images.\n",
        "            mask_dir (str): Path to the directory with corresponding masks.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = [f for f in os.listdir(image_dir) if f.endswith('.tif')]  # Filter .tif files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[idx])  # Assuming same filenames\n",
        "\n",
        "        # Load .tif images using tifffile\n",
        "        image = tifffile.imread(img_path)\n",
        "        mask = tifffile.imread(mask_path)\n",
        "\n",
        "        # Ensure the images are in the correct format\n",
        "        if len(image.shape) == 2:  # Grayscale image\n",
        "            image = np.stack([image] * 3, axis=-1)  # Convert to 3-channel RGB\n",
        "        if len(mask.shape) == 3:  # Multi-channel mask\n",
        "            mask = mask[:, :, 0]  # Use the first channel\n",
        "\n",
        "        # Convert to PIL Image for compatibility with torchvision transforms\n",
        "        image = Image.fromarray(image.astype(np.uint8))\n",
        "        mask = Image.fromarray(mask.astype(np.uint8))\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            image, mask = self.transform(image, mask)  # Pass both image and mask to the transform\n",
        "\n",
        "        # Normalize to [0, 1] (if not already done by transforms)\n",
        "        image = transforms.functional.to_tensor(image)  # Converts to tensor and scales to [0, 1]\n",
        "        mask = transforms.functional.to_tensor(mask)    # Converts to tensor and scales to [0, 1]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Define transformations for input images (572x572) and masks (388x388)\n",
        "class ApplyTransform:\n",
        "    def __init__(self, image_transform, mask_transform):\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        image = self.image_transform(image)\n",
        "        mask = self.mask_transform(mask)\n",
        "        return image, mask\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize input images to 572x572\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize masks to 388x388\n",
        "])\n",
        "\n",
        "transform = ApplyTransform(image_transform, mask_transform)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = PolypDataset(image_dir=\"CVC-ClinicDB/Original\", mask_dir=\"CVC-ClinicDB/Ground Truth\", transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.6 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Example usage\n",
        "for images, masks in train_loader:\n",
        "    print(f\"Images shape: {images.shape}\")  # Expected: (batch_size, 3, 572, 572)\n",
        "    print(f\"Masks shape: {masks.shape}\")   # Expected: (batch_size, 1, 388, 388)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvyvb7MRPkkI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return torch.sigmoid(self.conv(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "# Example usage\n",
        "model = UNet(in_channels=3, out_channels=1)\n",
        "#print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot2cmJDXPkkJ",
        "outputId": "a86a3bba-fc4f-4794-da37-377cdab8ef40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet(\n",
            "  (encoder1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (encoder2): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (encoder3): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (encoder4): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (bottleneck): Sequential(\n",
            "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv4): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (decoder4): Sequential(\n",
            "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (decoder3): Sequential(\n",
            "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (decoder2): Sequential(\n",
            "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (decoder1): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Real Unet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, init_features=64):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "\n",
        "        # Encoder (Contracting Path)\n",
        "        self.encoder1 = self._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = self._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = self._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = self._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        # Decoder (Expansive Path)\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = self._block(features * 16, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = self._block(features * 8, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = self._block(features * 4, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = self._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        # Final Layer\n",
        "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        # Final Layer\n",
        "        return self.conv(dec1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, kernel_size=3, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(features, features, kernel_size=3, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "# Example usage\n",
        "model = UNet(in_channels=3, out_channels=1)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeIu5DD1PkkK"
      },
      "outputs": [],
      "source": [
        "def iou(pred, target):\n",
        "    smooth = 1.0\n",
        "    pred_flat = pred.view(-1)\n",
        "    target_flat = target.view(-1)\n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    union = pred_flat.sum() + target_flat.sum() - intersection\n",
        "    return (intersection + smooth) / (union + smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEZgr-QlPkkK",
        "outputId": "ef8bd726-6743-4e65-8871-a17244f81303"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:21<00:00,  3.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.8407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:23<00:00,  3.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10, Loss: 0.7989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:20<00:00,  3.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10, Loss: 0.7812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:21<00:00,  3.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10, Loss: 0.7725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:21<00:00,  3.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10, Loss: 0.7636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:22<00:00,  3.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10, Loss: 0.7562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:21<00:00,  3.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10, Loss: 0.7499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:22<00:00,  3.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10, Loss: 0.7438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:24<00:00,  3.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10, Loss: 0.7384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [02:24<00:00,  3.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, Loss: 0.7334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, masks in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHmbRgj0PkkL"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), \"unet_polyp_segmentation.pth\")\n",
        "\n",
        "# Load the model\n",
        "model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "model.load_state_dict(torch.load(\"unet_polyp_segmentation.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3LQBcESPkkL"
      },
      "outputs": [],
      "source": [
        "def iou_score(pred, target):\n",
        "    smooth = 1.0\n",
        "    pred_flat = pred.view(-1)\n",
        "    target_flat = target.view(-1)\n",
        "\n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    union = pred_flat.sum() + target_flat.sum() - intersection\n",
        "\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "\n",
        "\n",
        "def dice_coefficient(pred, target):\n",
        "    smooth = 1.0\n",
        "    pred_flat = pred.view(-1)\n",
        "    target_flat = target.view(-1)\n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    return (2.0 * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cON9PXlSPkkM",
        "outputId": "022b27ef-0c21-4796-e57c-ad3d6d36ca78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Dice Coefficient: 0.1599\n",
            "Average IoU: 0.0877\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "dice_scores = []\n",
        "iou_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in val_loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        preds = torch.sigmoid(outputs) > 0.5  # Convert logits to binary mask\n",
        "\n",
        "        # Compute metrics\n",
        "        dice = dice_coefficient(preds, masks)\n",
        "        iou = iou_score(preds, masks)\n",
        "\n",
        "        # Append scores\n",
        "        dice_scores.append(dice.item())\n",
        "        iou_scores.append(iou.item())\n",
        "\n",
        "# Compute average scores\n",
        "avg_dice = sum(dice_scores) / len(dice_scores)\n",
        "avg_iou = sum(iou_scores) / len(iou_scores)\n",
        "\n",
        "print(f\"Average Dice Coefficient: {avg_dice:.4f}\")\n",
        "print(f\"Average IoU: {avg_iou:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metrics here is not comparable to research papers because the network is different from the actual Unet architecture. With the actual Unet architecture, my kernel crashes even when I reduce the training size."
      ],
      "metadata": {
        "id": "zRdJSBVKP1oA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cgcnn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}